{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0avHX2pRXN3L"
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xf-NnY5XXS-b"
   },
   "outputs": [],
   "source": [
    "# Step 1: Generate Functions and Taylor Expansions\n",
    "def generate_dataset(n_samples=1000):\n",
    "    x = sp.Symbol('x')\n",
    "    functions = [sp.sin(x), sp.cos(x), sp.exp(x), sp.log(1+x), x**2, x**3]\n",
    "    dataset = []\n",
    "    for _ in range(n_samples):\n",
    "        f = np.random.choice(functions)  # Randomly select a function\n",
    "        taylor_expansion = sp.series(f, x, 0, 5).removeO()\n",
    "        dataset.append((str(f), str(taylor_expansion)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rORQYQpAXYn4"
   },
   "outputs": [],
   "source": [
    "dataset = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "eea90b7ce32649439615366ccb307011",
      "506604457b464d969f21e5854da0adb9",
      "32173eb89d6e4e9fb979d32834924f29",
      "f2b1a159a2734fa7a5521edf96269120",
      "14499880487d4967933f0d60456f1f73",
      "9a4754bc916846c6b51f708dd31488fa",
      "d38261aa2f0142bb8d4f1876a0e46cbb",
      "7b396c5b600648509a05e7f6b5be7bb6",
      "8c7724871c0140a98b21d10c1158db1f",
      "59101e99ee2e4ae5bedf5c721961a643",
      "11e111910df34ab0a81d1133966bf07c",
      "d9b7af0be0df4702a08233f0facbc1a8",
      "3ba2660b01b64d46a165147170764baa",
      "ea1975aa7a604246a90d87ed93a67fe5",
      "2e6a80bcdcfd42e2a61bad67661fd4d4",
      "e1a5f7479ca5473aa12a10cc09aa0459",
      "391c764c8862499397d7a37415d2c962",
      "3ac2bab092a448babbc045eb0ec8165f",
      "49e022bb489e4cf2a5afc85dae7d2a34",
      "6392eb3b69ea47cd92e1cb50e2691471",
      "80e12038cbc6427e8dcb77a23f200fc2",
      "0f92284d75e44ad8a57ef7f6562987e6",
      "9b8ad9acc7a0400dba2c0f6730406ba2",
      "6625b919b1f249adb6ec9989ba492f75",
      "7c6b29724be74151985d1bcbf0c24608",
      "0b542dcf01034cf68fe68f00e61f10ac",
      "52194d3c2e674a679dba007904161c66",
      "fa9d44da2a994850a6cb7a7e938827e9",
      "c501671827824ff4a0d704d1ab987752",
      "7c039c175aaf45d895a3f48ef1197f2f",
      "b43dc03c1b3f4d1299251016c108abc7",
      "32c65fe6551349dc94ec4e0675b55a1b",
      "5b12d5b15b6d4b5ea56d06fc603717d9",
      "b7e38cc1087947ff98a0c2cd20f6b28a",
      "f0f516d954b84533a34d3dfe921aed8b",
      "7bae7369bab54b4e8885592eace65991",
      "238bfd4d40de4fa7a8b3762efa8a2b13",
      "ace30df0c5364c238388e21c1a8904ca",
      "15ad192be9db4cc4a41fbba07899cdbc",
      "f0931b29e2564350a91e7cd63500c162",
      "62b68efbe753414ebb04b63adb44e8f5",
      "a378278937614d5f8643f0bd404bb8c7",
      "b4ddc06243714bf7be41f7d0f04b1821",
      "ff53425f3de24dc08923db2f86d60aff",
      "499e1a8fae0e4aa68c96855e20795a18",
      "611f7fb6ff4a4179bd25c31a2246df5c",
      "ac210fce1967420fb28606a010c2abfa",
      "722ab5cb4d3148f48cdef8a68c65cb03",
      "a859410bb4c8433c9a051ee713c78094",
      "3b49c07344b041fd9f1cef20a962235d",
      "7aa9d2b6cbac401385e25a2c0524191e",
      "09e05d668f8242ed9cb2e02112d0a432",
      "777f969379de4b8990ca17a2e2f6743b",
      "305fbc1e15d341ee88517e6753d2c334",
      "aa9b07d3ef75469e8eb3c3b9a41b8685"
     ]
    },
    "id": "A6LR-zNqXb67",
    "outputId": "1bc0152f-60fe-4ca5-d050-7fa0a013c9db"
   },
   "outputs": [],
   "source": [
    "# Step 2: Tokenization\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(equations):\n",
    "    return tokenizer(equations, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fsGYg6eXfLu"
   },
   "outputs": [],
   "source": [
    "X, Y = zip(*dataset)\n",
    "X_tokens = tokenize_function(list(X))\n",
    "Y_tokens = tokenize_function(list(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pvqpRk6Xh3q"
   },
   "outputs": [],
   "source": [
    "# Step 3: Dataset Preparation\n",
    "class EquationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs[\"input_ids\"].long()\n",
    "        self.outputs = outputs[\"input_ids\"].long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.inputs[idx], \"labels\": self.outputs[idx]}\n",
    "\n",
    "dataset = EquationDataset(X_tokens, Y_tokens)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ro4Daz2aXotv"
   },
   "outputs": [],
   "source": [
    "# Step 4: Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM expects (batch_size, seq_length, input_size)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Take the last LSTM output\n",
    "\n",
    "lstm_model = LSTMModel(1, 256, 128) # Adjust input_size to 1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4_GGQy37Wnw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79C6icnVXsgq",
    "outputId": "8f783830-42ea-476d-d61a-123a182f4805"
   },
   "outputs": [],
   "source": [
    "def train_lstm(model, dataloader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[\"input_ids\"].float()  # Shape: (batch_size, seq_length)\n",
    "            labels = batch[\"labels\"].float()\n",
    "\n",
    "            print(f\"Input shape before LSTM: {inputs.shape}\")\n",
    "            # The input to the LSTM should be 3D: (batch_size, seq_length, input_size)\n",
    "            # We assume input_size is 1 for now, you may need to adjust this.\n",
    "            # inputs = inputs.unsqueeze(-1)  # This line is no longer needed\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Pass the input to the model\n",
    "            outputs = model(inputs)\n",
    "            # Reshape the outputs and labels for the loss function\n",
    "            # outputs will now be (batch_size, seq_length, output_size)\n",
    "            # We need to reshape it to (batch_size * seq_length, output_size) for MSELoss\n",
    "            outputs = outputs.reshape(-1, outputs.shape[-1]) # -1 infers the batch_size * seq_length\n",
    "            labels = labels.reshape(-1, labels.shape[-1])    # Reshape labels similarly\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "train_lstm(lstm_model, train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "zJP-VQWJX2Sm",
    "outputId": "d3b5b9bc-4255-49ba-f8b9-c118d9f9f6ba"
   },
   "outputs": [],
   "source": [
    "# Step 5: Define Transformer Model\n",
    "transformer_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "transformer_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=transformer_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qf-D76nKX6Dz",
    "outputId": "9cda6010-af8b-4b02-9b94-98207ea66353"
   },
   "outputs": [],
   "source": [
    "# Step 6: Evaluate Transformer Model\n",
    "def generate_taylor_expansion(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = transformer_model.generate(input_ids, max_length=50)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "test_prompt = \"sin(x)\"\n",
    "predicted_expansion = generate_taylor_expansion(test_prompt)\n",
    "print(\"Predicted Taylor Expansion:\", predicted_expansion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "cCkhmnFIX62Z",
    "outputId": "adaa1106-36ea-4e8a-edeb-6bdf2321c016"
   },
   "outputs": [],
   "source": [
    "# Step 7: Visualization\n",
    "train_loss = trainer.state.log_history\n",
    "losses = [entry['loss'] for entry in train_loss if 'loss' in entry]\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
